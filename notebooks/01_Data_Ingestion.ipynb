{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion and Cleaning\n",
    "\n",
    "In this notebook, we will create a pipeline to:\n",
    "* Ingest the data \n",
    "* Format the data into useable types for ML\n",
    "* Clean the data - deal with messy/missing data\n",
    "* Normalize/Standardize the data as needed - get the data into a more efficient set of values\n",
    "\n",
    "First, we will look at each of the pieces of the pipeline individually, followed by a pipeline to make this easier in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.  Data Ingestion\n",
    "\n",
    "There are many formats we could use to ingest data.  I don't have an API key for my brokerage account, so I have to resort to downloading the data in a fairly messy, manual manner.  The easiest route results in a .csv file, so we will use that for the time being.  \n",
    "\n",
    "Fortunately, it is not a huge dataset, so does not need to utilize any technologies for big data, such as Spark, or a database backend.\n",
    "\n",
    "In the future, I would like to create a backend chron job to pull data using an API key, but Chase is not responding to my request for one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
