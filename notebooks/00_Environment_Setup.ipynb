{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will ensure our environment is set up for:  \n",
    "* Basic Python, GPU install checks \n",
    "* Experiment Tracking with Weights and Biases \n",
    "* GitHub integration\n",
    "* GPU availability\n",
    "\n",
    "This all assumes you have used the requirements.txt to install the required libraries into your python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tests\n",
    "* Python version\n",
    "* Ensure we can use our GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be using Python 3.10.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the NVIDIA drivers are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 16 11:16:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 551.76       CUDA Version: 12.4     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090 Ti     On  | 00000000:09:00.0  On |                  Off |\n",
      "| 44%   39C    P5              28W / 450W |   2711MiB / 24564MiB |     12%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        22      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out PyTorch Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is quite useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "PyTorch version: 2.3.0+cu121\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)\n",
      "Python platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.0.76\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090 Ti\n",
      "Nvidia driver version: 551.76\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                       x86_64\n",
      "CPU op-mode(s):                     32-bit, 64-bit\n",
      "Address sizes:                      48 bits physical, 48 bits virtual\n",
      "Byte Order:                         Little Endian\n",
      "CPU(s):                             12\n",
      "On-line CPU(s) list:                0-11\n",
      "Vendor ID:                          AuthenticAMD\n",
      "Model name:                         AMD Ryzen 5 3600 6-Core Processor\n",
      "CPU family:                         23\n",
      "Model:                              113\n",
      "Thread(s) per core:                 2\n",
      "Core(s) per socket:                 6\n",
      "Socket(s):                          1\n",
      "Stepping:                           0\n",
      "BogoMIPS:                           7200.01\n",
      "Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr virt_ssbd arat umip rdpid\n",
      "Hypervisor vendor:                  Microsoft\n",
      "Virtualization type:                full\n",
      "L1d cache:                          192 KiB (6 instances)\n",
      "L1i cache:                          192 KiB (6 instances)\n",
      "L2 cache:                           3 MiB (6 instances)\n",
      "L3 cache:                           16 MiB (1 instance)\n",
      "Vulnerability Gather data sampling: Not affected\n",
      "Vulnerability Itlb multihit:        Not affected\n",
      "Vulnerability L1tf:                 Not affected\n",
      "Vulnerability Mds:                  Not affected\n",
      "Vulnerability Meltdown:             Not affected\n",
      "Vulnerability Mmio stale data:      Not affected\n",
      "Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\n",
      "Vulnerability Spec rstack overflow: Mitigation; safe RET\n",
      "Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\n",
      "Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\n",
      "Vulnerability Srbds:                Not affected\n",
      "Vulnerability Tsx async abort:      Not affected\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] mypy-extensions==1.0.0\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] optree==0.11.0\n",
      "[pip3] pytorch-lightning==2.2.4\n",
      "[pip3] torch==2.3.0\n",
      "[pip3] torchaudio==2.3.0\n",
      "[pip3] torchinfo==1.8.0\n",
      "[pip3] torchmetrics==1.4.0\n",
      "[pip3] torchutils==0.0.4\n",
      "[pip3] torchvision==0.18.0\n",
      "[pip3] triton==2.3.0\n",
      "[conda] libmagma                  2.7.2                h173bb3b_2    conda-forge\n",
      "[conda] libmagma_sparse           2.7.2                h173bb3b_3    conda-forge\n",
      "[conda] libtorch                  2.1.2           cuda120_h86db2e7_300    conda-forge\n",
      "[conda] magma                     2.7.2                h51420fd_3    conda-forge\n",
      "[conda] mkl                       2023.2.0         h84fe81f_50496    conda-forge\n",
      "[conda] numpy                     1.24.4          py310ha4c1d20_0    conda-forge\n",
      "[conda] optree                    0.11.0                   pypi_0    pypi\n",
      "[conda] pytorch-lightning         2.2.4                    pypi_0    pypi\n",
      "[conda] torch                     2.3.0                    pypi_0    pypi\n",
      "[conda] torchaudio                2.3.0                    pypi_0    pypi\n",
      "[conda] torchinfo                 1.8.0                    pypi_0    pypi\n",
      "[conda] torchmetrics              1.4.0                    pypi_0    pypi\n",
      "[conda] torchutils                0.0.4                    pypi_0    pypi\n",
      "[conda] torchvision               0.18.0                   pypi_0    pypi\n",
      "[conda] triton                    2.3.0                    pypi_0    pypi\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import collect_env\n",
    "print(collect_env.main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out Tensorflow Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Tensorflow, the first call always generates a lot of information, most of which is not of concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 11:14:32.078986: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-16 11:14:32.079042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-16 11:14:32.104979: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-16 11:14:32.143351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 11:14:35.113540: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-16 11:14:35.120913: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-16 11:14:35.120965: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:09:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('cpu_compiler',\n",
       "              '/home/conda/feedstock_root/build_artifacts/tensorflow-split_1706532345435/_build_env/bin/x86_64-conda-linux-gnu-gcc'),\n",
       "             ('cuda_compute_capabilities',\n",
       "              ['sm_60',\n",
       "               'sm_70',\n",
       "               'sm_75',\n",
       "               'sm_80',\n",
       "               'sm_86',\n",
       "               'sm_89',\n",
       "               'sm_90',\n",
       "               'compute_90']),\n",
       "             ('cuda_version', '12.0'),\n",
       "             ('cudnn_version', '8'),\n",
       "             ('is_cuda_build', True),\n",
       "             ('is_rocm_build', False),\n",
       "             ('is_tensorrt_build', False)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sysconfig.get_build_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and Biases \n",
    "\n",
    "For experiment tracking, model archive (similar to Tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.require(\"core\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# do this in a seperate terminal window\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
